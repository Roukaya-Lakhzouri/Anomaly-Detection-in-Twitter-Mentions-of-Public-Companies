# ğŸ“Œ Anomaly Detection in Twitter Mentions of Public Companies

### ğŸ” Overview

This project implements an **anomaly detection pipeline** for time-series data on Twitter mentions of major publicly traded companies.
The workflow is fully reproducible with **DVC (Data Version Control)**, covering data collection, preprocessing, model training, and evaluation.

---

### ğŸ“‚ Dataset

* Source: [Kaggle â€“ Anomaly Detection in Time Series](https://www.kaggle.com/code/ahmadihossein/anomaly-detection-in-time-series/input)
* Description: Twitter mentions of large publicly traded companies (every 5 minutes).
* Companies included:

  * Apple
  * Amazon
  * Salesforce
  * CVS
  * Facebook
  * Google
  * IBM
  * Coca-Cola
  * Pfizer
  * UPS

The metric is the **number of mentions per ticker symbol per 5-minute interval**.

---

### âš™ï¸ Pipeline Stages

#### **Stage 1: Data Collection**

* Script: `src/data_collection.py`
* Output: `data/raw/`
* Collects raw Twitter mention data.

#### **Stage 2: Data Preparation**

* Script: `src/data_prep.py`
* Input: `data/raw/` â†’ Output: `data/processed/`
* Cleans and preprocesses the raw data (scaling, feature engineering).

#### **Stage 3: Model Building & Training**

* Script: `src/model_building.py`
* Input: `data/processed/` â†’ Output: `model.pkl`
* Labels anomalies using a **3-sigma statistical rule**.
* Trains a **RandomForestClassifier** to detect anomalies.
* Saves trained model (`model.pkl`).

#### **Stage 4: Model Testing**

* Script: `src/model_testing.py`
* Input: `model.pkl` â†’ Output: `anomaly_results.json`
* Loads the trained model.
* Evaluates on test data using **Accuracy, Precision, Recall, F1-score**.
* Saves metrics in structured JSON.

---

### ğŸ“Š Tools & Libraries

* **DVC** â†’ pipeline reproducibility and data versioning.
* **scikit-learn** â†’ preprocessing, Random Forest, evaluation.
* **pandas, numpy** â†’ data handling and feature engineering.
* **joblib** â†’ saving and loading trained models.
* **JSON** â†’ saving evaluation results.

---

### ğŸ—‚ Project Structure

```bash
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/               # raw data (collected via Stage 1)
â”‚   â”œâ”€â”€ processed/         # processed data (Stage 2)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_collection.py # data collection logic
â”‚   â”œâ”€â”€ data_prep.py       # preprocessing & feature engineering
â”‚   â”œâ”€â”€ model_building.py  # anomaly labeling & model training
â”‚   â”œâ”€â”€ model_testing.py   # model evaluation and metrics export
â”‚
â”œâ”€â”€ model.pkl              # trained RandomForestClassifier (output of Stage 3)
â”œâ”€â”€ anomaly_results.json   # evaluation metrics (output of Stage 4)
â”œâ”€â”€ dvc.yaml               # DVC pipeline definition
â”œâ”€â”€ requirements.txt       # Python dependencies
â””â”€â”€ README.md              # project documentation
```

---

### ğŸš€ How to Run

1. Clone the repo and install dependencies:

   ```bash
   pip install -r requirements.txt
   ```

2. Initialize DVC (first time only):

   ```bash
   dvc init
   ```

3. Run the entire pipeline:

   ```bash
   dvc repro
   ```

4. Visualize the DAG of stages:

   ```bash
   dvc dag
   ```

---

### âœ… Outputs

* **`model.pkl`** â†’ trained anomaly detection model.
* **`anomaly_results.json`** â†’ evaluation metrics (accuracy, precision, recall, f1-score).
* **`dvc.yaml`** â†’ reproducible ML pipeline.
